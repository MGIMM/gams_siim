---
title: "Visulization of variance estimator"
author: ""
date: ""
output: 
    html_document:
      toc: true
      toc_depth: 4
      number_sections: false

runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Dynamics

The associated dynamic is the same as the one dimentional example in GAMS.pdf: the Euler discretization of the following diffusion process

$$
dX_t = - \mu \Delta t + \sqrt{2 \beta^{-1}} dW_t
$$

#### Numerical settings:

$$
\Delta t = 0.01,\qquad \beta = 1, \qquad \mu = 1, \qquad X_0 = 1
$$

#### Rare event:

$$
a = 0.1,\qquad b = 4.9,\qquad z_{max} = b
$$

### GAMS settings

* resampling strategy: __multinomial__, __keep survived__ (only resample the particle under and equals the current level)
* number of particles at each step $n_{rep} = 100$.
* the minimum number of killed particles at each step $k = 1,5,30,50,70$.
* test function: $\varphi(x) = \mathbf{1}_{\{T_A(x) < T_B(x)\}}$

$$
E = \mathbb{E}[\varphi(X)] = \mathbb{P}(T_A(X) < T_B(X))
$$

#### GAMS estimator

$$
\widehat{E} = \prod_{q = 0}^{Q_{iter}} \frac{n_{rep}-K^{(q)}}{n_{rep}} \cdot\frac{1}{n_{rep}} \sum_{i=1}^{n_{rep}}\varphi(X^{(i,Q_{iter})}) 
$$
where
$$
K^{(q)} = card (I_{off})
$$
the number of killed particles at each step and $Q_{iter}$ the stopping time of GAMS algorithm

$$
Q_{iter} :=\min\left( \inf\left\{q\in \mathbb{N}: \Xi(X^{(q)}) > z_{max}\right\},\inf\left\{q \in \mathbb{N}:K^{(q)} = n_{rep}\right\}\right)
$$

#### Implementation

[Source code](https://github.com/MGIMM/gams_sim/blob/master/py/gams_origin.py)

### Variance estimator

The two estimators we implemented are as follows

#### Multinomial resampling strategy

This is exactly the same variance estimator of LEE & WHITELEY.

$$
\widehat{V} =
\widehat{E}^2 - \frac{n_{rep}^{Q_{iter} -1}}{(n_{rep}-1)^{Q_{iter}+1}}
\cdot \left(\prod_{q=0}^{Q_{iter}} 
\frac{n_{rep} - K^{(q)}}{n_{rep}}\right)^2
\cdot
\sum_{E_{Q_{iter}^i} \neq E_{Q_{iter}^j}}\varphi(X^{(i,Q_{iter})})\varphi(X^{(j,Q_{iter})})
$$

where $E_{Q_{iter}}^i$ denotes the index of ancestor of $X^{(i,Q_{iter})}$ at step 0.

#### Keep survived resampling strategy

This is a modified version of the variance estimator introduced by LEE & WHITELEY. 


$$
\widehat{V} =
\widehat{E}^2 - \frac{n_{rep}^{Q_{iter} -1}}{(n_{rep}-1)^{Q_{iter}+1}}
\cdot\sum_{E_{Q_{iter}^i} \neq E_{Q_{iter}^j}}\varphi(X^{(i,Q_{iter})})\varphi(X^{(j,Q_{iter})})
\cdot S(i,j)
$$
where

$$
S(i,j) = \prod_{q_1\in B_S(i,j)} \frac{n_{rep} - K^{(q_1)}}{n_{rep}}\cdot\frac{n_{rep} - K^{(q_1)}-1}{n_{rep}-1}
\cdot\frac{n_{rep}-1}{n_{rep}}
\cdot \prod_{q_2\in [0:Q_{iter}]\backslash B_S(i,j)} \left(\frac{n_{rep} - K^{(q_2)}}{n_{rep}}\right)^2
$$
and
$$
B_s(i,j) = \left\{ q = 0,\dots,Q_{iter} : G_{\Xi(X^{(q)})}(X^{(A_{q,Q_{iter}}^i,q)}) = G_{\Xi(X^{(q)})}(X^{(A_{q,Q_{iter}}^j,q)}) = 1 \right\}
$$

$A_{q,Q_{iter}}^i$ indicates the index of parent of $X^{(i,Q_{iter})}$ in step $q$ and 
particularly, we have $E_{Q_{iter}}^i = A_{0,Q_{iter}}^i$.

### Visualization

```{r mul, echo=FALSE}

library(ggplot2)
library(rjson)
inputPanel(
  selectInput("s_start", label = "Start of n_sim:",
              choices = c(1, 200, 1000, 10000, 20000), selected = 20),
  
  selectInput("s_method", label = "Resampling strategy:",
              choices = c('keep_survived','multinomial'), selected = 'keep_survived'),
  selectInput("k_test", label = "k",
              choices = c(1,5,30,50,70), selected = '30'),
  
  sliderInput("s_size", label = "Size of sample:",
              min = 10, max = 30000, value = 200, step = 10)
)
  


renderPlot({
  
json_file <- paste0('../py/json/original_',
                    as.character(input$s_method),
                    '_n_rep_100_k_',
                    as.character(input$k_test),
                    '_n_sim_50000_a_0.1_b_4.9_beta_1.0_dt_0.01_mu_1.0.json')
json_data <- fromJSON(file=json_file)


V_list <- json_data["V_list"][[1]]
E_list <- json_data["E_list"][[1]]
  

sample_start = as.numeric(input$s_start)
sample_size = as.numeric(input$s_size)

V_list_sample <- V_list[sample_start:(sample_start+sample_size-1)]
E_list_sample <- E_list[sample_start:(sample_start+sample_size-1)]

v_mean <- cumsum(V_list[1:(sample_start+sample_size-1)])/1:(sample_start+sample_size-1)
v_mean <- v_mean[sample_start:(sample_start+sample_size-1)]

v_std <- sapply(1:sample_size, function(k) {return(sd(V_list_sample[1:k]))})


e <- qnorm(0.975)*v_std
e <- e/sample_start:(sample_start+sample_size-1)

v_naive <- sapply(1:(sample_start+sample_size-1), function(k) {return(var(E_list[1:k]))})
v_naive <- v_naive[sample_start:(sample_start+sample_size-1)]

df = data.frame(v_mean, v_mean-e, v_mean +e, v_naive, sample_start:(sample_start+sample_size-1))
colnames(df) = c('v_mean','ci_lower','ci_upper', 'v_naive','n_sim')

theme_set(theme_bw())
theme_update(panel.background = element_rect(fill = 'white', colour = "white")
       ,panel.grid.major = element_line(colour = 'grey80',linetype='dotted')
       ,panel.grid.minor = element_line(colour = 'grey80',linetype='dotted')
       ,legend.box.background = element_rect(colour = 'white'))


ggplot(data= df,aes(x = n_sim))+
  geom_line(aes(y = v_mean, colour = 'mean of variance estimator',
                linetype = 'mean of variance estimator'),
  )+
  geom_line(aes(y = ci_lower, colour = 'lower bound of 95% CI',
                linetype = 'lower bound of 95% CI'),
  )+
  geom_line(aes(y = ci_upper, colour = 'upper bound of 95% CI',
                linetype = 'upper bound of 95% CI'),
  )+
  geom_line(aes(y = v_naive, colour = 'naive variance estimator',
                linetype = 'naive variance estimator'),
  )+
  xlab('number of simulations')+
  ylab('values')+
  scale_colour_manual('legend',breaks = c('mean of variance estimator',
                                          'lower bound of 95% CI',
                                          'upper bound of 95% CI',
                                          'naive variance estimator'),
                        values = c('mean of variance estimator'='black',
                                   'lower bound of 95% CI'='darkred' ,
                                   'upper bound of 95% CI'='darkred',
                                   'naive variance estimator'='darkgreen') )+
  scale_linetype_manual('legend',breaks = c('mean of variance estimator',
                                          'lower bound of 95% CI',
                                          'upper bound of 95% CI',
                                          'naive variance estimator'),
                        values = c('mean of variance estimator'='solid',
                                   'lower bound of 95% CI'='dotted' ,
                                   'upper bound of 95% CI'='dotted',
                                   'naive variance estimator'='longdash') )

  
  
})

```

**logfile :**

```{r log, echo = FALSE}
inputPanel(
  selectInput("s_method_log", label = "Resampling strategy:",
              choices = c('keep_survived','multinomial'), selected = 'keep_survived'),
  selectInput("k_test_log", label = "k",
              choices = c(1,5,30,50,70), selected = '30')
)
  


renderPrint({
  
log_file <- paste0('../py/log/original_',
                    as.character(input$s_method_log),
                    '_n_rep_100_k_',
                    as.character(input$k_test_log),
                    '_n_sim_50000_a_0.1_b_4.9_beta_1.0_dt_0.01_mu_1.0.log')
print(readLines(log_file))

  
})
```

